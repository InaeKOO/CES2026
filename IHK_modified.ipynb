{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import numpy as np\n",
    "import pickle\n",
    "import re\n",
    "import csv\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import ast\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from pypfopt import EfficientFrontier\n",
    "from pypfopt import risk_models\n",
    "from pypfopt import expected_returns\n",
    "from pypfopt import objective_functions\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Model Architecture: iTransformer (Inverted Transformer)\n",
    "# =============================================================================\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.depth = d_model // num_heads\n",
    "        self.W_Q = nn.Linear(d_model, d_model)\n",
    "        self.W_K = nn.Linear(d_model, d_model)\n",
    "        self.W_V = nn.Linear(d_model, d_model)\n",
    "        self.W_O = nn.Linear(d_model, d_model)\n",
    " \n",
    "    def forward(self, Q, K, V):\n",
    "        Q, K, V = self.W_Q(Q), self.W_K(K), self.W_V(V)\n",
    "        Q, K, V = self._split_heads(Q), self._split_heads(K), self._split_heads(V)\n",
    "        \n",
    "        scores = torch.matmul(Q, K.transpose(-1, -2)) / (self.depth ** 0.5)\n",
    "        attn = torch.softmax(scores, dim=-1)\n",
    "        \n",
    "        return self.W_O(self._combine_heads(torch.matmul(attn, V)))\n",
    " \n",
    "    def _split_heads(self, x):\n",
    "        return x.view(x.size(0), -1, self.num_heads, self.depth).transpose(1, 2)\n",
    " \n",
    "    def _combine_heads(self, x):\n",
    "        x = x.transpose(1, 2).contiguous()\n",
    "        return x.view(x.size(0), -1, self.num_heads * self.depth)\n",
    "\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super().__init__()\n",
    "        self.attention = MultiHeadAttention(d_model, num_heads)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(d_model, 4 * d_model), nn.ReLU(), nn.Linear(4 * d_model, d_model)\n",
    "        )\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    " \n",
    "    def forward(self, x):\n",
    "        x = self.norm1(x + self.attention(x, x, x))\n",
    "        return self.norm2(x + self.ffn(x))\n",
    "\n",
    "\n",
    "class iTransformer(nn.Module):\n",
    "    \"\"\"\n",
    "    iTransformer: Inverted Transformer for Time Series Forecasting\n",
    "    \n",
    "    Key insight: Treats each variable as a token (not each time step).\n",
    "    - Input:  (Batch, SeqLen, NumVars)\n",
    "    - Output: (Batch, PredLen, NumVars)\n",
    "    \"\"\"\n",
    "    def __init__(self, input_len, output_len, num_features, hidden_dim, num_heads, num_layers):\n",
    "        super().__init__()\n",
    "        self.input_proj = nn.Linear(input_len, hidden_dim)  # Project time dim to hidden\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, num_features, hidden_dim))\n",
    "        self.encoder = nn.ModuleList([EncoderLayer(hidden_dim, num_heads) for _ in range(num_layers)])\n",
    "        self.output_proj = nn.Linear(hidden_dim, output_len)  # Project hidden to pred_len\n",
    " \n",
    "    def forward(self, x):\n",
    "        # x: (B, L, N) -> transpose -> (B, N, L) -> project -> (B, N, D)\n",
    "        x = self.input_proj(x.permute(0, 2, 1)) + self.pos_embed\n",
    "        \n",
    "        for layer in self.encoder:\n",
    "            x = layer(x)\n",
    "        \n",
    "        # (B, N, D) -> project -> (B, N, P) -> transpose -> (B, P, N)\n",
    "        return self.output_proj(x).permute(0, 2, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Config] Device: cuda, Features: 50, Input: 504d, Output: 14d\n",
      "\n",
      "[1/5] Loading data...\n",
      "      Price data: 1256 days x 50 features\n",
      "[2/5] Computing returns & splitting data...\n",
      "      Train: 1004d, Test: 251d\n",
      "[3/5] Creating sliding windows...\n",
      "      Samples: 487, Shape: [487, 504, 50]\n",
      "[4/5] Training model (100 epochs)...\n",
      "      Epoch  20/100: Loss = 0.626030\n",
      "      Epoch  40/100: Loss = 0.391078\n",
      "      Epoch  60/100: Loss = 0.223439\n",
      "      Epoch  80/100: Loss = 0.137127\n",
      "      Epoch 100/100: Loss = 0.091201\n",
      "[5/5] Saving model and data...\n",
      "\n",
      "[Done] All artifacts saved.\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Configuration\n",
    "# =============================================================================\n",
    "CSV_FILES = [\n",
    "    'BROADCOM 5년치.csv', 'ALPHABET C 5년치.csv', 'AMAZON 5년치.csv', \n",
    "    'APPLE 5년치.csv', 'META 5년치.csv', 'MICROSOFT 5년치.csv', \n",
    "    'NETFLIX 5년치.csv', 'NVIDIA 5년치.csv', 'PALANTIR 5년치.csv', 'TESLA 5년치.csv'\n",
    "]\n",
    "FEATURE_COLS = ['Close/Last', 'Volume', 'Open', 'High', 'Low']\n",
    "STOCK_NAMES = [re.match(r'^[A-Z]+', f).group(0) for f in CSV_FILES]\n",
    "\n",
    "INPUT_LEN = 504      # ~2 years of trading days\n",
    "OUTPUT_LEN = 14      # 14-day prediction horizon\n",
    "HIDDEN_DIM = 128\n",
    "NUM_HEADS = 8\n",
    "NUM_LAYERS = 6\n",
    "NUM_EPOCHS = 100\n",
    "BATCH_SIZE = 16\n",
    "TRAIN_RATIO = 0.8\n",
    "\n",
    "NUM_FEATURES = len(CSV_FILES) * len(FEATURE_COLS)  # 10 stocks * 5 features = 50\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(f\"[Config] Device: {DEVICE}, Features: {NUM_FEATURES}, Input: {INPUT_LEN}d, Output: {OUTPUT_LEN}d\")\n",
    "\n",
    "# =============================================================================\n",
    "# 1. Load and Merge Data\n",
    "# =============================================================================\n",
    "print(\"\\n[1/5] Loading data...\")\n",
    "all_data_dfs = []\n",
    "column_names = []\n",
    "\n",
    "for csv_file in CSV_FILES:\n",
    "    stock_name = re.match(r'^[A-Z]+', csv_file).group(0)\n",
    "    df = pd.read_csv(csv_file, parse_dates=['Date'], index_col='Date').sort_index()\n",
    "    \n",
    "    cleaned_cols = {}\n",
    "    for col in FEATURE_COLS:\n",
    "        series = df[col]\n",
    "        if series.dtype == 'object':\n",
    "            series = series.str.replace(r'[$,]', '', regex=True)\n",
    "        series = pd.to_numeric(series, errors='coerce')\n",
    "        col_name = f\"{stock_name}_{col}\"\n",
    "        cleaned_cols[col_name] = series\n",
    "        column_names.append(col_name)\n",
    "    all_data_dfs.append(pd.DataFrame(cleaned_cols))\n",
    "\n",
    "price_df = pd.concat(all_data_dfs, axis=1).dropna()\n",
    "print(f\"      Price data: {price_df.shape[0]} days x {price_df.shape[1]} features\")\n",
    "\n",
    "# =============================================================================\n",
    "# 2. Convert to Returns & Split\n",
    "# =============================================================================\n",
    "print(\"[2/5] Computing returns & splitting data...\")\n",
    "returns_df = price_df.pct_change().dropna()\n",
    "returns_np = np.nan_to_num(returns_df.to_numpy(dtype=np.float32), nan=0.0, posinf=0.0, neginf=0.0)\n",
    "\n",
    "train_size = int(len(returns_np) * TRAIN_RATIO)\n",
    "train_returns = returns_np[:train_size]\n",
    "test_returns = returns_np[train_size:]\n",
    "\n",
    "scaler = StandardScaler().fit(train_returns)\n",
    "train_scaled = scaler.transform(train_returns)\n",
    "\n",
    "print(f\"      Train: {len(train_returns)}d, Test: {len(test_returns)}d\")\n",
    "\n",
    "# =============================================================================\n",
    "# 3. Create Sliding Windows\n",
    "# =============================================================================\n",
    "print(\"[3/5] Creating sliding windows...\")\n",
    "inputs, outputs = [], []\n",
    "for i in range(len(train_scaled) - INPUT_LEN - OUTPUT_LEN + 1):\n",
    "    inputs.append(train_scaled[i : i + INPUT_LEN])\n",
    "    outputs.append(train_scaled[i + INPUT_LEN : i + INPUT_LEN + OUTPUT_LEN])\n",
    "\n",
    "inputs = torch.tensor(np.array(inputs), dtype=torch.float32)\n",
    "outputs = torch.tensor(np.array(outputs), dtype=torch.float32)\n",
    "data_loader = DataLoader(TensorDataset(inputs, outputs), batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "print(f\"      Samples: {len(inputs)}, Shape: {list(inputs.shape)}\")\n",
    "\n",
    "# =============================================================================\n",
    "# 4. Train Model\n",
    "# =============================================================================\n",
    "print(f\"[4/5] Training model ({NUM_EPOCHS} epochs)...\")\n",
    "model = iTransformer(INPUT_LEN, OUTPUT_LEN, NUM_FEATURES, HIDDEN_DIM, NUM_HEADS, NUM_LAYERS).to(DEVICE)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for x_batch, y_batch in data_loader:\n",
    "        x_batch, y_batch = x_batch.to(DEVICE), y_batch.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(model(x_batch), y_batch)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    if (epoch + 1) % 20 == 0:\n",
    "        print(f\"      Epoch {epoch+1:3d}/{NUM_EPOCHS}: Loss = {total_loss/len(data_loader):.6f}\")\n",
    "\n",
    "# =============================================================================\n",
    "# 5. Save Artifacts\n",
    "# =============================================================================\n",
    "print(\"[5/5] Saving model and data...\")\n",
    "torch.save(model.state_dict(), \"unified_model.pt\")\n",
    "with open(\"unified_scaler.pkl\", 'wb') as f: pickle.dump(scaler, f)\n",
    "with open(\"unified_column_map.pkl\", 'wb') as f: pickle.dump(column_names, f)\n",
    "with open(\"unified_returns.pkl\", 'wb') as f: pickle.dump(returns_df, f)\n",
    "with open(\"unified_prices.pkl\", 'wb') as f: pickle.dump(price_df, f)\n",
    "with open(\"unified_config.pkl\", 'wb') as f: pickle.dump({\n",
    "    'stock_names': STOCK_NAMES, 'column_names': column_names,\n",
    "    'input_len': INPUT_LEN, 'output_len': OUTPUT_LEN,\n",
    "    'hidden_dim': HIDDEN_DIM, 'num_features': NUM_FEATURES\n",
    "}, f)\n",
    "\n",
    "print(\"\\n[Done] All artifacts saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/4] Loading saved artifacts...\n",
      "      Loaded model: input=504d, output=14d\n",
      "[2/4] Predicting future returns...\n",
      "[3/4] Computing expected returns...\n",
      "\n",
      "======================================================================\n",
      "Stock           Current  Predicted   14d Return       Annual\n",
      "======================================================================\n",
      "BROADCOM     $   369.63 $   383.35       +3.71%       +92.7%\n",
      "ALPHABET     $   281.82 $   260.23       -7.66%       -76.2%\n",
      "AMAZON       $   244.22 $   242.01       -0.90%       -15.1%\n",
      "APPLE        $   270.37 $   261.66       -3.22%       -44.5%\n",
      "META         $   648.35 $   774.89      +19.52%     +2375.7%\n",
      "MICROSOFT    $   517.81 $   510.51       -1.41%       -22.5%\n",
      "NETFLIX      $  1118.86 $  1284.15      +14.77%     +1094.4%\n",
      "NVIDIA       $   202.49 $   213.82       +5.60%      +166.4%\n",
      "PALANTIR     $   200.47 $   208.61       +4.06%      +104.6%\n",
      "TESLA        $   456.56 $   484.65       +6.15%      +192.9%\n",
      "======================================================================\n",
      "\n",
      "[4/4] Optimizing portfolio...\n",
      "\n",
      "Optimal Portfolio Weights:\n",
      "------------------------------\n",
      "  BROADCOM       1.63%\n",
      "  META          61.71%\n",
      "  NETFLIX       27.94%\n",
      "  NVIDIA         3.20%\n",
      "  PALANTIR       1.50%\n",
      "  TESLA          4.02%\n",
      "\n",
      "Expected Performance:\n",
      "------------------------------\n",
      "Expected annual return: 1788.0%\n",
      "Annual volatility: 37.1%\n",
      "Sharpe Ratio: 48.14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ihkoo/miniconda/envs/deep/lib/python3.14/site-packages/pypfopt/efficient_frontier/efficient_frontier.py:259: UserWarning: max_sharpe transforms the optimization problem so additional objectives may not work as expected.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Inference: Load Model & Optimize Portfolio\n",
    "# =============================================================================\n",
    "\n",
    "# --- Load Artifacts ---\n",
    "with open(\"unified_config.pkl\", 'rb') as f: cfg = pickle.load(f)\n",
    "with open(\"unified_scaler.pkl\", 'rb') as f: scaler = pickle.load(f)\n",
    "with open(\"unified_returns.pkl\", 'rb') as f: returns_df = pickle.load(f)\n",
    "with open(\"unified_prices.pkl\", 'rb') as f: price_df = pickle.load(f)\n",
    "\n",
    "DEVICE = torch.device('cpu')\n",
    "model = iTransformer(\n",
    "    cfg['input_len'], cfg['output_len'], cfg['num_features'], \n",
    "    cfg['hidden_dim'], num_heads=8, num_layers=6\n",
    ").to(DEVICE)\n",
    "model.load_state_dict(torch.load(\"unified_model.pt\", map_location=DEVICE, weights_only=True))\n",
    "model.eval()\n",
    "\n",
    "# --- Predict Future Returns ---\n",
    "returns_np = np.nan_to_num(returns_df.to_numpy(dtype=np.float32), nan=0.0, posinf=0.0, neginf=0.0)\n",
    "scaled_input = scaler.transform(returns_np)[-cfg['input_len']:]\n",
    "input_tensor = torch.tensor(scaled_input, dtype=torch.float32).unsqueeze(0).to(DEVICE)\n",
    "\n",
    "with torch.no_grad():\n",
    "    scaled_pred = model(input_tensor).squeeze(0).cpu().numpy()\n",
    "predicted_returns = scaler.inverse_transform(scaled_pred)\n",
    "\n",
    "# --- Compute Expected Returns ---\n",
    "current_prices = price_df.iloc[-1]\n",
    "predicted_mus = {}\n",
    "results = []\n",
    "\n",
    "for stock in cfg['stock_names']:\n",
    "    col = f\"{stock}_Close/Last\"\n",
    "    idx = cfg['column_names'].index(col)\n",
    "    cum_ret = np.prod(1 + predicted_returns[:, idx]) - 1\n",
    "    pred_price = current_prices[col] * (1 + cum_ret)\n",
    "    annual_mu = (1 + cum_ret) ** (252.0 / cfg['output_len']) - 1\n",
    "    predicted_mus[stock] = annual_mu\n",
    "    results.append([stock, current_prices[col], pred_price, cum_ret * 100, annual_mu * 100])\n",
    "\n",
    "# --- Display Price Predictions ---\n",
    "print(\"=\" * 60)\n",
    "print(\" PRICE PREDICTIONS (14-day horizon)\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"{'Stock':<10} {'Now':>9} {'Pred':>9} {'14d Ret':>9} {'Annual':>10}\")\n",
    "print(\"-\" * 60)\n",
    "for r in results:\n",
    "    print(f\"{r[0]:<10} ${r[1]:>7.2f}  ${r[2]:>7.2f}  {r[3]:>+7.2f}%  {r[4]:>+8.1f}%\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# --- Portfolio Optimization ---\n",
    "close_df = price_df[[f\"{s}_Close/Last\" for s in cfg['stock_names']]]\n",
    "close_df.columns = cfg['stock_names']\n",
    "S = risk_models.sample_cov(close_df, frequency=252)\n",
    "mu_series = pd.Series(predicted_mus)\n",
    "\n",
    "ef = EfficientFrontier(mu_series, S)\n",
    "ef.add_objective(objective_functions.L2_reg, gamma=10.0)\n",
    "ef.max_sharpe(risk_free_rate=0.02)\n",
    "weights = ef.clean_weights()\n",
    "\n",
    "# --- Display Portfolio Weights ---\n",
    "print(\"\\n\" + \"=\" * 40)\n",
    "print(\" OPTIMAL PORTFOLIO ALLOCATION\")\n",
    "print(\"=\" * 40)\n",
    "sorted_weights = sorted(weights.items(), key=lambda x: -x[1])\n",
    "for stock, w in sorted_weights:\n",
    "    bar = \"#\" * int(w * 30)\n",
    "    if w > 0.001:\n",
    "        print(f\"{stock:<10} {w*100:>6.2f}% |{bar}\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# --- Performance Metrics ---\n",
    "perf = ef.portfolio_performance(risk_free_rate=0.02)\n",
    "print(f\"\\nExpected Return: {perf[0]*100:.1f}%  |  Volatility: {perf[1]*100:.1f}%  |  Sharpe: {perf[2]:.2f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
